{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keyword",
      "provenance": [],
      "authorship_tag": "ABX9TyMXGM4gtPvak1rtlA2D+rce",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangjinsu/keyword-ai/blob/main/keyword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYdeT5yyeq9N",
        "outputId": "07801b7f-d616-4751-e8e9-b1a5af039ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.16.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "7suLUaJ0gNUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "황금올리브치킨 BBQ\n",
        "by GeniusJW 2018. 10. 12. 21:46\n",
        "\n",
        "\n",
        "\n",
        "안녕하세요 GeniusJW 입니다. 오늘 소개 해 드릴 메뉴는, BBQ 황금올리브 치킨 입니다. 황금올리브치킨은 BBQ 에서 제가 종종 주문 해 먹는 치킨입니다. 후라이드치킨 중에는 기름도 가장 깨끗한 편이고, 맛있어서 제가 즐겨먹는 메뉴인데요,\n",
        "\n",
        "\n",
        "\n",
        "황금올리브치킨은 특별한 건 없고, 단지 튀김옷이 바삭하면서 얇은 것이 특징입니다. 다른 치킨가게는 튀김옷이 생각보다 두꺼운 편이고 기름을 많이 머금고 있어서 느끼한 편인데 반면 BBQ 황금올리브 치킨은 생각보단 느끼하지 않고, 올리브오일 향이 은은하게 배어나오는 게 괜찮더라구요.\n",
        "\n",
        "\n",
        "\n",
        "BBQ 치킨 외에도 후라이드 치킨은 대부분 맛 있는 편이지만, 프랜차이즈 치킨 가운데서는 가장 준수한 맛인 것 같습니다. 써프라이드나 다른 메뉴는 솔직히 개인적으로 만족스럽진 않지만, 황금올리브 치킨 만큼은 인정하는 맛입니다.\n",
        "\n",
        "\n",
        "\n",
        "오랜만에 황금올리브 치킨을 주문하여 먹었는데요, 맛도 괜찮고 가격도 괜찮아서 앞으로 당분간 황금올리브 치킨 가격이 오르지 않는 이상 종종 주문 해 먹을 것 같습니다. 요즘은 9시 넘어서 KFC 치킨을 먹으면 1+1 이라 주로 KFC 치킨을 먹었는데, KFC는 튀김옷도 약간 두꺼운 편이고, 기름이랑 짠 맛도 좀 강해서 BBQ를 개인적으로 더 선호하는데요, 아무래도 가성비인 KFC도 포기하진 못할 것 같습니다..ㅋㅋㅋ(BBQ도 좋고 KFC도 좋고)\n",
        "\"\"\"\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_doc = okt.pos(doc)\n",
        "\n",
        "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Adjective' and word[0][-1] not in ['다', '요']])\n",
        "\n",
        "print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
        "print('명사 추출 :',tokenized_nouns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69z_I3D8gYMd",
        "outputId": "e89158e5-62c3-446a-f81f-b30453bf3f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('\\n', 'Foreign'), ('황금', 'Noun'), ('올리브', 'Noun'), ('치킨', 'Noun'), ('BBQ', 'Alpha'), ('\\n', 'Foreign'), ('by', 'Alpha'), ('GeniusJW', 'Alpha'), ('2018', 'Number'), ('.', 'Punctuation')]\n",
            "명사 추출 : 깨끗한 맛있어서 특별한 없고 얇은 두꺼운 있어서 느끼한 느끼하지 은은하게 있는 솔직히 만족스럽진 괜찮고 괜찮아서 두꺼운 강해서 좋고 좋고\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_range = (1, 1)\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('trigram 개수 :',len(candidates))\n",
        "print('trigram 다섯개만 출력 :',candidates[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1T-_cmogjzB",
        "outputId": "4e646825-3fc8-443c-f11d-43ea5ddfbb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram 개수 : 17\n",
            "trigram 다섯개만 출력 : ['강해서' '괜찮고' '괜찮아서' '깨끗한' '느끼하지']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "metadata": {
        "id": "h1e79lJlgnWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQTCNpnUgqOD",
        "outputId": "f17c7579-87cb-4052-c2fb-a2c4e3790ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['괜찮아서', '깨끗한', '만족스럽진', '은은하게', '맛있어서']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "    # 문서와 각 키워드들 간의 유사도\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "\n",
        "    # 각 키워드들 간의 유사도\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "\n",
        "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ],
      "metadata": {
        "id": "cU9nzR_riS1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pikrf3ebiYty",
        "outputId": "7a9a6e04-ad58-44f5-e112-52b45e187bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['얇은', '느끼하지', '강해서', '깨끗한', '은은하게']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQjNHZbpj2pZ",
        "outputId": "e42412ab-2a4c-4dbb-a8a6-6ae27f885089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['없고', '있는', '두꺼운', '느끼하지', '은은하게']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "\n",
        "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
        "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "\n",
        "    # 각 키워드들 간의 유사도\n",
        "    word_similarity = cosine_similarity(candidate_embeddings)\n",
        "\n",
        "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # keywords_idx = [2]\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "\n",
        "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
        "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
        "    for _ in range(top_n - 1):\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # MMR을 계산\n",
        "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # keywords & candidates를 업데이트\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "WjELe7WDj_Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SscyrSigkBD0",
        "outputId": "840371e0-195c-4da2-f4b9-6018ec62306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['맛있어서', '은은하게', '깨끗한', '만족스럽진', '두꺼운']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmbx1sdwkCcf",
        "outputId": "a4b6f437-f630-4be1-db39-6e4124a0621e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['맛있어서', '없고', '은은하게', '두꺼운', '얇은']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}